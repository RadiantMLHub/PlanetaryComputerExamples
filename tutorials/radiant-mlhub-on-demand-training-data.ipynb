{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cf916c-0991-4af6-889f-a1ce86696d46",
   "metadata": {},
   "source": [
    "## On Demand Training Data from Radiant MLHub and Planetary Computer\n",
    "\n",
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b5b89-32c4-4f6b-81fb-f41d782d251f",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the process of requesting on-demand traning data from the [Planetary Computer Data Catalog](https://planetarycomputer.microsoft.com/catalog) to pair with the [BigEarthNet](https://mlhub.earth/data/bigearthnet_v1) dataset downloaded from Radiant MLHub. This is an important workflow for someone in the geospatial community who wants to train an ML model on a datasource outside of a prepackaged dataset, such as those found on MLHub. They can start with any dataset containing source image and label collections in STAC, obtain a random sample to work with, fetch source images from a different collection or satellite product, and then reproject and crop those images to match the spatial and temporal extent of the original dataset.\n",
    "\n",
    "**NOTE:** because the workflow documented below uses libraries like `pystac_client` and `stackstac`, the datasets queried need to be organized into STAC Collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130b365-6fff-4d2a-86a9-39085ab13886",
   "metadata": {},
   "source": [
    "Let's start by importing the Python libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7b447-cf8a-4cc7-aaed-4e8407d0f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pillow wget graphviz # not installed on PC by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e144460-5549-4ab4-ba98-10a1a7ebd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "from radiant_mlhub import Collection\n",
    "import planetary_computer\n",
    "from pystac_client import Client as ps_client\n",
    "from pystac import ItemCollection, Item, Asset\n",
    "from dask.distributed import Client as dd_client\n",
    "from dask import delayed, compute, persist\n",
    "\n",
    "import numpy as np\n",
    "from stackstac import stack\n",
    "from geopandas import GeoDataFrame\n",
    "import rasterio as rio\n",
    "import rioxarray\n",
    "from xarray import DataArray\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb87b-9d9c-4140-bac8-3e95f146c029",
   "metadata": {},
   "source": [
    "### Define global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2f657-3229-4037-bf9c-541c503cc269",
   "metadata": {},
   "source": [
    "In addition to the API key, we will also need to define some other initial global variables to get our workflow started. e.g. a temporary working directory to download and write data to, the STAC API endpoints, names of Collections, and other variables like the RGB bands for those collections. These are pretty flexible depending on your individual needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf07a1-3a4a-4207-a449-7be766fa7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary working directory on local machine or PC instance\n",
    "TMP_DIR = tempfile.gettempdir()\n",
    "\n",
    "# API endpoints for MLHub and Planetary Computer catalogs\n",
    "MLHUB_API_URL = \"https://api.radiant.earth/mlhub/v1\"\n",
    "MSPC_API_URL = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "\n",
    "# Names of Collections that will be queried against using pystac_client\n",
    "BIGEARTHNET_SOURCE_COLLECTION = \"bigearthnet_v1_source\"  # sentinel-2 source imagery\n",
    "BIGEARTHNET_LABEL_COLLECTION = \"bigearthnet_v1_labels\"  # geojson classification labels\n",
    "PLANETARY_COMPUTER_LANDSAT_8 = \"landsat-8-c2-l2\"  # landsat 8 source imagery on PC\n",
    "\n",
    "# Default variables that will be used in the API queries\n",
    "BIGEARTHNET_TIME_RANGE = \"2017-06-01/2018-05-31\"  # full date range for BigEarthNet\n",
    "LABEL_CRS = CRS(\"EPSG:4326\")\n",
    "DATE_BUFFER = 60\n",
    "LANDSAT_8_RGB_BANDS = [\"SR_B4\", \"SR_B3\", \"SR_B2\"]  # names of RGB bands from BigEarthNet\n",
    "BIGEARTHNET_RGB_BANDS = [\"B04\", \"B03\", \"B02\"]  # names of RGB bands from PC Landsat 8\n",
    "\n",
    "# Bounding box for demonstration fetching Items over Luxembourg\n",
    "LUXEMBOURG_AOI = [6.06, 49.58, 6.21, 49.66]  # aoi around Luxembourg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e31b3-5cde-4a7b-af43-dc194b06d0a0",
   "metadata": {},
   "source": [
    "### Authentication with Radiant MLHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11e821-b98b-4df1-a26c-826c9bdbec50",
   "metadata": {},
   "source": [
    "Programmatic access to the Radiant MLHub API using the `pystac_client` library requires both the API end-point and an API key. You can obtain an API key for free by registering an account on [mlhub.earth](https://mlhub.earth/). This can be found under `Settings & API Key` from the drop-down once logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9dd60-3abc-464d-af25-4b23c0d2783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLHUB_API_KEY = getpass.getpass(prompt=\"MLHub API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad2439-109f-4eef-ac3b-dac65f54e3aa",
   "metadata": {},
   "source": [
    "Once you have your API key, you will need to create a default profile by setting up a .mlhub/profiles file in your home directory. You can use the `mlhub configure` command line tool to do this:\n",
    "\n",
    "`$ mlhub configure`<br>\n",
    "API Key: {<i>Enter your API key here</i>}<br>\n",
    "Wrote profile to /home/jovyan/.mlhub/profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77d029-191e-42a5-8250-bc451b80f247",
   "metadata": {},
   "source": [
    "### Configure API connection to Radiant MLHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0480635-eef5-4e3f-847a-5060c409ae4f",
   "metadata": {},
   "source": [
    "This makes a connection to the Radiant MLHub Data Catalog using the API endpoint URL, and the API key from your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79c301-76df-4158-bf97-3da53552e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlhub_catalog = ps_client.open(\n",
    "    url=MLHUB_API_URL, parameters={\"key\": MLHUB_API_KEY}, ignore_conformance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637734bc-af77-4c81-ab26-98e808de6415",
   "metadata": {},
   "source": [
    "### Fetch label items from BigEarthNet over Luxembourg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7d7b2-7fed-4412-8c9b-448baad6e595",
   "metadata": {},
   "source": [
    "This helper function below encapsulates the process of querying a STAC API endpoint to fetch an ItemCollection matching query criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567a337-d7da-441d-9a2a-0fccde0ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_stac_api(\n",
    "    catalog_client: ps_client,\n",
    "    collections: List[str],\n",
    "    bbox: List[float] = None,\n",
    "    datetime: str = None,\n",
    "    ids: List[str] = None,\n",
    ") -> ItemCollection:\n",
    "    \"\"\"Uses a pystac client to query a STAC API endpoint for Items.\n",
    "    Searching using either IDs or datetime and bbox params\n",
    "\n",
    "    Args:\n",
    "        catalog_client: an instance of pystac_client.Client\n",
    "        collections: a list of string names matching valid STAC Collections\n",
    "        bbox: a list of floats specifying [xmin, ymin, xmax, ymax] values\n",
    "        datetime: a string representing a single datetime or date range\n",
    "        cloud_cover: a float value representation of image covered by clouds\n",
    "\n",
    "    Returns:\n",
    "        ItemCollection: pystac collection (iterable) of items found\n",
    "    \"\"\"\n",
    "    if ids:\n",
    "        search = catalog_client.search(collections=collections, ids=ids)\n",
    "    elif bbox and datetime:\n",
    "        search = catalog_client.search(\n",
    "            collections=collections, bbox=bbox, datetime=datetime\n",
    "        )\n",
    "    else:\n",
    "        search = catalog_client.search(collections=collections)\n",
    "\n",
    "    return search.get_all_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f19ce4-57fd-43d0-9263-7a5edd106ee8",
   "metadata": {},
   "source": [
    "We will now use the API client with the helper function above to get label Items over Luxembourg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b5f79-3988-4d97-a6ef-58287341ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_label_items = search_stac_api(\n",
    "    catalog_client=mlhub_catalog,\n",
    "    collections=[BIGEARTHNET_LABEL_COLLECTION],\n",
    "    bbox=LUXEMBOURG_AOI,\n",
    "    datetime=BIGEARTHNET_TIME_RANGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c392c-9ae5-43e2-b7e0-71dd9f749cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(origin_label_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d121a9-e54b-4039-9cef-04277962c2ca",
   "metadata": {},
   "source": [
    "This is another helper function that simply displays the geometry for labels from an ItemCollection overlayed on a map of the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f203e31-5b09-4f2e-bf27-9a3ef8e3fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_search_extent(items: ItemCollection) -> None:\n",
    "    \"\"\"Extracts geometry from ItemCollection to display polygons on a map.\n",
    "\n",
    "    Args:\n",
    "        items: ItemCollection of Items retrieved from pystac_client search\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame object with the .explore() method called\n",
    "    \"\"\"\n",
    "    item_feature_collection = items.to_dict()\n",
    "    geom_df = GeoDataFrame.from_features(item_feature_collection).set_crs(4326)\n",
    "    return geom_df[[\"geometry\", \"datetime\"]].explore(\n",
    "        column=\"datetime\", style_kwds={\"fillOpacity\": 0.2}, cmap=\"viridis\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab923855-3c20-4f10-af00-d4175a54fdd4",
   "metadata": {},
   "source": [
    "Here are the BigEarthNet chips with their bounding boxes that matched the spatial parameters for the city of Luxembourg and surrounding areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bf5d8-8dc8-491d-b3cd-1ea1263774ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explore_search_extent(origin_label_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6e607-c79b-4678-a483-9bacb0b3b1df",
   "metadata": {},
   "source": [
    "### Download the entire label collection for BigEarthNet from Radiant MLHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e131160-50bb-487f-8fcb-96d37ce80167",
   "metadata": {},
   "source": [
    "We could certainly use the method above to query label Items directly from our connection to the Radiant MLHub API endpoint. However, on very large collections, such as in the case with BigEarthNet, pagination becomes a bottleneck issue in obtaining and resolving STAC items, as it only returns 100 items at a time.  Querying the entire Collection of nearly ~600,000 Items could take hours.\n",
    "\n",
    "Therefore, downloading the label Collection (which is only 160 MB) directly is preferrable to paginating over the entire Collection using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f67824-bf8f-4cce-862e-856d9cfed26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_collection_path = os.path.join(\n",
    "    TMP_DIR, BIGEARTHNET_LABEL_COLLECTION, \"collection.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd7e52-6bc7-4a8d-bf56-060e80f4019b",
   "metadata": {},
   "source": [
    "Check if collection folder already exists before downloading 173 mb dataset. Otherwise download and uncompress the `.tar.gz` file to extract the label collection files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5a8c4-c604-4f8a-8fb9-1d90607206ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(label_collection_path):\n",
    "    collection = Collection.fetch(BIGEARTHNET_LABEL_COLLECTION)\n",
    "    archive_path = collection.download(TMP_DIR)\n",
    "    !tar -xf {archive_path.as_posix()} -C {TMP_DIR}\n",
    "else:\n",
    "    print(\"Archive file already downloaded from Radiant MLHub, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2683b-edde-43e0-8bf2-b791a8e04dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigearthnet_dir = os.listdir(os.path.join(TMP_DIR, BIGEARTHNET_LABEL_COLLECTION))\n",
    "bigearthnet_dir[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c28041-79f1-4ebd-a09a-14d440ac2757",
   "metadata": {},
   "source": [
    "This is the total count of label Item (chip) directories, plus one for the STAC Collection itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f270583-b7a5-4e37-8a50-2f9f4e49fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigearthnet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d358d-54ff-4300-89e5-43f16e452574",
   "metadata": {},
   "source": [
    "### Obtain a random sample of label Items from BigEarthNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ad5d5-458f-4f56-88d9-574e5c37ba26",
   "metadata": {},
   "source": [
    "We don't want to work with the entire dataset of nearly 600,000 labels. This would take too long to download, and we likely won't have enough disk space or space in memory, so let's work with a random sample of the dataset that is 10% of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ad2d8-7cbd-4bc4-a1d2-f6bcd0c9fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(label_collection_path)\n",
    "with open(label_collection_path, \"r\") as in_file:\n",
    "    collection_data = json.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b0c28-9dee-4f83-b3fe-a49079a530dd",
   "metadata": {},
   "source": [
    "This confirms we have all of the label Items STAC objects and image data from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffac3d1-4cf1-4bf8-b066-6de4fa1f4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_item_links = [\n",
    "    link[\"href\"] for link in collection_data[\"links\"] if link[\"rel\"] == \"item\"\n",
    "]\n",
    "len(label_item_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9836f40-4228-4d72-8b84-d94cae1030c6",
   "metadata": {},
   "source": [
    "Now we take a random sample that is 1/100th the original dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8f303-08c8-4353-847e-fc4d712edac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_item_sample = np.random.choice(\n",
    "    a=label_item_links, size=int(len(label_item_links) / 100), replace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c171723-8bff-4755-aa56-2db7d2074011",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_item_sample[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e290e-f7e4-4906-a44c-0435f6ce8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(len(label_item_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b5914-ddf8-4332-87f9-4e631e71110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_item = Item.from_file(\n",
    "    os.path.join(TMP_DIR, BIGEARTHNET_LABEL_COLLECTION, label_item_sample[rand_idx])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65959f74-4d1d-4ce4-8583-bc51d6047ce1",
   "metadata": {},
   "source": [
    "Chip ID for the sample label Item pulled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420d40b-2cd3-4f74-b271-a137ca71e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_item.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0734b-ce42-48de-8bea-f4c4f5e9b37f",
   "metadata": {},
   "source": [
    "Links for the sample label Item, take special note of the `rel=source` Link listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d94501-6305-4329-aa4f-ef69802951b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_item.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23fb83-400e-487b-940f-fc158b5ae41b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fetch source items for random sample from BigEarthNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419d96f-f8b9-4911-b5d1-4a4077767062",
   "metadata": {},
   "source": [
    "If we had the source collection archive downloaded and uncompressed in the same parent directory as the labels collection, we could reference the source Items and images directly. However the BigEarthNet source collection is over 60GB when compressed. Therefore to work around the disk size limitations of a Planetary Computer instance, we can query the same source items from the MLHub API endpoint, the same way we got the labels, but filter to the exact source item using IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa334c57-9dbe-495e-b040-980adf1192c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_item_ids(label_item: Item) -> List[str]:\n",
    "    return [\n",
    "        link.href.split(\"/\")[-2] for link in label_item.links if link.rel == \"source\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d25a1-da18-4020-9a0d-f6d0af6c898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_source_items = search_stac_api(\n",
    "    catalog_client=mlhub_catalog,\n",
    "    collections=[BIGEARTHNET_SOURCE_COLLECTION],\n",
    "    ids=get_source_item_ids(first_label_item),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e775fa-dd60-4623-8d44-3d83beafcb2c",
   "metadata": {},
   "source": [
    "This is the number of source items that match the query parameters we sent to the MLHub API using the first label's bounding box and datetime properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98a5d6-1a55-466a-b768-4433cea148ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(origin_source_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba4eaf-03d8-4e56-92c8-4e98b26ba983",
   "metadata": {},
   "source": [
    "Taking a look at some of the properties of the first source Item found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854abe53-e1b7-452a-8aa2-c139aa220348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for source_item in origin_source_items:\n",
    "    print(source_item.id)\n",
    "    print(source_item.datetime)\n",
    "    print(source_item.bbox)\n",
    "    print(source_item.properties)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303475f-d29b-4e84-82ef-d355ef0519de",
   "metadata": {},
   "source": [
    "With the properties from this sample source Item, we can observe where the chip is located, the relevant Sentinel-2 bands (assets) and datetime the image was captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d88ffe-f530-46d6-87d9-4337c1ec0202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explore_search_extent(origin_source_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77109ba6-f0a1-426e-8884-39a5fac2795f",
   "metadata": {},
   "source": [
    "This is the location of the source items fetched from the label Items sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0cbba-32b8-47ad-9913-e7ba2a939922",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fetch Landsat 8 scenes based on source Item bbox and datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392b23e-eebb-4a53-88d8-f49fbfacfaf1",
   "metadata": {},
   "source": [
    "Configure API connection for the microsoft planetary computer stac endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddda32-d50d-413d-add8-dedaa2f9a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_buffer(item_datetime: str, date_delta: int) -> str:\n",
    "    \"\"\"Takes a datetime string and returns a buffer around that date\n",
    "\n",
    "    Args:\n",
    "        item_datetime: string of the datetime property from an Item\n",
    "        date_delta: integer for days to add before and after a date\n",
    "\n",
    "    Returns:\n",
    "        a string range representing the full date buffer\n",
    "    \"\"\"\n",
    "    delta = td(days=date_delta)\n",
    "    item_dt = dt.strptime(item_datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    dt_start = item_dt - delta\n",
    "    dt_start_str = dt_start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    dt_end = item_dt + delta\n",
    "    dt_end_str = dt_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return f\"{dt_start_str}/{dt_end_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e4ec0-a1b8-490c-b732-4c10d89b06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cloud_cover_scene(label_geom: Polygon, search_items: ItemCollection) -> Item:\n",
    "    \"\"\"Finds the Item with minimal cloud cover from an ItemCollection\n",
    "\n",
    "    Args:\n",
    "        label_geom: Polygon geometry to ensure label completely within scene\n",
    "        search_items: ItemCollection of the Items found from pystac_client search\n",
    "\n",
    "    Returns:\n",
    "        Item where label completely contained within, and minimal cloud cover\n",
    "    \"\"\"\n",
    "    min_cc = np.inf\n",
    "    min_cc_item = None\n",
    "    for item in search_items:\n",
    "        item_geom = shape(item.geometry)\n",
    "        item_cc = item.properties[\"eo:cloud_cover\"]\n",
    "        if item_cc < min_cc and label_geom.within(item_geom):\n",
    "            min_cc = item_cc\n",
    "            min_cc_item = item\n",
    "    return min_cc_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b23fa5-d588-42d4-b913-fab7819e7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landsat_8_match(label_item: Item) -> Tuple[Item, Item]:\n",
    "    \"\"\"Finds the best Landsat 8 match using source Item datetime and bounding box.\n",
    "\n",
    "    Args:\n",
    "        label_item: the STAC label Item object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the BigEarthNet source Item and the Landsat 8 match Item\n",
    "    \"\"\"\n",
    "    # get the matching source Item properties\n",
    "    source_items = search_stac_api(\n",
    "        catalog_client=mlhub_catalog,\n",
    "        collections=[BIGEARTHNET_SOURCE_COLLECTION],\n",
    "        ids=get_source_item_ids(label_item),\n",
    "    )\n",
    "\n",
    "    if source_items:\n",
    "        source_item = source_items[0]\n",
    "        source_bbox = source_item.bbox\n",
    "        source_datetime = source_item.properties[\"datetime\"]\n",
    "\n",
    "        # search PC Catalog for L8 Items\n",
    "        l8_items = search_stac_api(\n",
    "            catalog_client=mspc_catalog,\n",
    "            collections=PLANETARY_COMPUTER_LANDSAT_8,\n",
    "            bbox=source_bbox,\n",
    "            datetime=temporal_buffer(source_datetime, DATE_BUFFER),\n",
    "        )\n",
    "\n",
    "        # filter to best L8 Item match\n",
    "        signed_l8_items = planetary_computer.sign(l8_items)\n",
    "        best_l8_match = min_cloud_cover_scene(\n",
    "            shape(source_item.geometry), signed_l8_items\n",
    "        )\n",
    "\n",
    "        if not best_l8_match:\n",
    "            print(\n",
    "                \"No Landsat 8 Item was found on the Planetary \"\n",
    "                \"Computer matching the query parameters:\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Source Item ID: {source_item.id} \"\n",
    "                f\"Bbox: {source_bbox}, \"\n",
    "                f\"Datetime: {source_datetime}\"\n",
    "            )\n",
    "            best_l8_match = None\n",
    "    else:\n",
    "        print(\n",
    "            \"No Sentinel-2 source Item was found in the \"\n",
    "            \"BigEarthNet dataset matching that label item!\"\n",
    "        )\n",
    "        source_item = None\n",
    "    return source_item, best_l8_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad7ef1-143e-47f0-b6b9-26c73e5cc65d",
   "metadata": {},
   "source": [
    "Since it is known that the BigEarthNet dataset from MLHub has a 1-to-1 pairing of source and labels, we can safely assume the first source item is the appropriate match for our label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77ef46-b54a-42e7-a780-bc8e8094ec6f",
   "metadata": {},
   "source": [
    "This makes a connection to the Planetary Computer Data Catalog using the API endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38280716-e130-4de4-9699-2c0bcbfc056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mspc_catalog = ps_client.open(MSPC_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f35d02-d2de-4be5-9317-c80214502c88",
   "metadata": {},
   "source": [
    "We will now use the API client with the helper function above to fetch the best Landsat 8 match for the sampled label Item. This will find only the scenes where the label is completely within the scene, and there is minimal cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3ab14-86c7-42a8-be96-8156f5e74d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_item, best_l8_match = get_landsat_8_match(first_label_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53556cf0-375b-4044-9262-d16de707a13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if best_l8_match:\n",
    "    print(best_l8_match.id)\n",
    "    print(best_l8_match.bbox)\n",
    "    print(best_l8_match.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b5d88-7f3c-4e74-8e68-2408ba762b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explore_search_extent(ItemCollection([best_l8_match]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c13807-50a3-4a8c-8a89-33002eafabe6",
   "metadata": {},
   "source": [
    "If everything worked correctly, the geographic scope of the Landsat 8 scene should encompass a much larger surface area than the Sentinel-2 source and label chips. From here we need to crop the image down and make sure the chips from both products match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a1d9e-55aa-4781-a0f5-e77ace273240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redirect_url(asset: Asset) -> str:\n",
    "    \"\"\"Returns the direct URL to an asset.\n",
    "\n",
    "    Args:\n",
    "        asset: Asset object from an Item\n",
    "\n",
    "    Returns:\n",
    "        string response URL direct to Asset\n",
    "    \"\"\"\n",
    "    response = requests.get(asset.href, allow_redirects=True)\n",
    "    if response.status_code == 200:\n",
    "        return response.url\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e977d-eb14-4aa9-85df-adbe84b1732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_stack = stack(\n",
    "    items=ItemCollection([source_item]),\n",
    "    assets=BIGEARTHNET_RGB_BANDS,\n",
    "    epsg=rio.open(get_redirect_url(source_item.assets[\"B02\"])).crs.to_epsg(),\n",
    "    resolution=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f9b13-82a3-4a5d-82f2-9ab5066c6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95ad09-34ff-4be1-a5a6-7b870578768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_stack[0].plot(col=\"band\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2326b7-d1cc-4474-82b6-95f7da05f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_original = stack(\n",
    "    items=ItemCollection([best_l8_match]), assets=LANDSAT_8_RGB_BANDS, resolution=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa276eef-73ea-4834-a6a1-7cbd205f3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20472b-be7e-481a-b1ed-3bbcff044c7f",
   "metadata": {},
   "source": [
    "As we can see from the metadata for the Xarray above, the Landsat 8 scene has a significantly larger geographic footprint, `~20,000 x ~20,000 pixels`, compared to `120 x 120 pixels` for the Sentinel-2 chips that were prepared for the dataset. We need to crop/mask the image down so they represent the exact same terrain.\n",
    "\n",
    "Luckily, the `bounds_latlon` parameter of `stackstac` makes it easy to crop the image to this size automatically for all bands/assets requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045f54d-5076-4a14-9292-3dbdde339cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_cropped = stack(\n",
    "    items=ItemCollection([best_l8_match]),\n",
    "    assets=LANDSAT_8_RGB_BANDS,\n",
    "    bounds_latlon=source_item.bbox,\n",
    "    resolution=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0600e6-3bb8-4da9-90b2-ee4ec45ea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5e313-0a2f-410e-8339-9630ca87dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_cropped[0].plot(col=\"band\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46c85b-daa9-44df-9be7-62dfa1234b25",
   "metadata": {},
   "source": [
    "Now we have a cropped Landsat 8 chip that spatially and temporally matches our Sentinel-2 source imagery and label sample from the BigEarthNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70b063-d273-4aee-864b-07e318388890",
   "metadata": {},
   "source": [
    "### Launch a Dask gateway cluster for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb650d80-bf1a-4aad-8f8b-08a612e28aae",
   "metadata": {},
   "source": [
    "We will use Dask to optimize our data processing of hundreds of Landsat-8 scenes by parallelizing the workflow with a delayed computation graph. The Dask Client schedules, runs the delayed computations, and gathers the results, while the Dask Gateway provides a secure and centralized way of managing the multiple client clusters. This is especially useful for running Dask on Planetary Computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29531759-6d19-4010-8401-eb947a32c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dd_client(\n",
    "    # you can configure Dask client parameters here\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a0496-a6c0-4aa2-9181-708904a5bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will only work on PC or a machine with gateway cluster configured\n",
    "# gateway = dask_gateway.Gateway()\n",
    "# options = gateway.cluster_options()\n",
    "# options[\"worker_cores\"] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b09697-b6ab-4026-bfcb-2f5214b03f5c",
   "metadata": {},
   "source": [
    "### Scale the workflow using Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368c39f-94a1-41ba-acc0-fd18c8dc1c18",
   "metadata": {},
   "source": [
    "These are two helper functions that we will use to encapsulate the process of creating the cropped Landsat 8 chips and write them to disk in parallel using the Dask Delayed decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924acb1-092e-4f86-b73f-5b56ccdebe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landsat_8_dataarray(item_path: str) -> DataArray:\n",
    "    \"\"\"Creates a Landsat 8 chip from BigEarthNet label chip.\n",
    "\n",
    "    Args:\n",
    "        item_path: string path to the label item on disk\n",
    "\n",
    "    Returns:\n",
    "        Landsat 8 DataArray that has been cropped to label bbox\n",
    "    \"\"\"\n",
    "    # read label Item object\n",
    "    label_item = Item.from_file(\n",
    "        os.path.join(TMP_DIR, BIGEARTHNET_LABEL_COLLECTION, item_path)\n",
    "    )\n",
    "\n",
    "    # fetch the Landsat 8 scene that best matches the label\n",
    "    s2_source, l8_match = get_landsat_8_match(label_item)\n",
    "\n",
    "    if l8_match:\n",
    "        # crop L8 match to S2 dims and read image data\n",
    "        l8_stack = stack(\n",
    "            items=ItemCollection([l8_match]),\n",
    "            assets=LANDSAT_8_RGB_BANDS,\n",
    "            bounds_latlon=s2_source.bbox,\n",
    "            resolution=10,\n",
    "        )\n",
    "\n",
    "        return l8_stack\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b974fe-0c6f-40a3-ad63-2f4753e0236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tifs_bands(l8_array: DataArray, l8_item_id: str) -> None:\n",
    "    \"\"\"Writes to a GeoTiff for each band in Landsat 8 DataArray\n",
    "\n",
    "    Args:\n",
    "        l8_array: the DataArray object created from the BigEarthNet label item\n",
    "    \"\"\"\n",
    "    # write cropped L8 DataArray to a tiff file for each band\n",
    "    for _band in LANDSAT_8_RGB_BANDS:\n",
    "        l8_band_img = l8_array.sel(band=_band)\n",
    "        l8_band_filename = os.path.join(\n",
    "            TMP_DIR, \"landsat_8_source\", l8_item_id, f\"{l8_item_id}_{_band}.tiff\"\n",
    "        )\n",
    "        Path(os.path.split(l8_band_filename)[0]).mkdir(parents=True, exist_ok=True)\n",
    "        l8_band_img[0].rio.to_raster(l8_band_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a97950-5b08-4e79-ab15-3736697d0584",
   "metadata": {},
   "source": [
    "This sets the stage for the Dask Task Scheduler by mapping all label Items to the `create_landsat_8_dataarray` function. Nothing in the task graph will actually be executed until the `.compute()` command is ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ce116-526a-4657-9c44-f51cffb9326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pool = []\n",
    "\n",
    "for item_path in label_item_sample:\n",
    "    delayed_task = delayed(create_landsat_8_dataarray)(item_path)\n",
    "    task_pool.append(delayed_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92137ec9-748b-4126-9984-11f4f8d6ec26",
   "metadata": {},
   "source": [
    "Now we will persist the objects into memory and run the computations to create our DataArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01785b-b490-456b-a6c7-3d439275773f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "task_pool = persist(*task_pool)\n",
    "task_pool = compute(*task_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95d76c-dc3e-4591-a596-a95e27a3dbde",
   "metadata": {},
   "source": [
    "Lastly, we want to write a GeoTIFF to disk for each band of each Landsat 8 DataArray we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1436d4d-10ac-4b39-88fb-5150fe9df12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for l8_array in task_pool:\n",
    "    if isinstance(l8_array, DataArray):\n",
    "        write_tifs_bands(l8_array, l8_array.id.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde6ce7-9d7a-4114-88ba-56e4a4bea247",
   "metadata": {},
   "source": [
    "This confirms that folders with images were written to disk. If there is a discrepancy between the sample size and the output, it's likely that there wasn't always a matching Landsat 8 scene given the geometry and datetime parameters for a particular Sentinel-2 source Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64a009-9a87-4fb2-bf5b-fbc41a3f8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_chip_dir = os.path.join(TMP_DIR, \"landsat_8_source\")\n",
    "len(os.listdir(landsat_chip_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0884796-bfab-4905-aa75-2f8dd43a5a13",
   "metadata": {},
   "source": [
    "Open one of the new Landsat 8 chips to inspect what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3d5a8-2677-43dc-867a-153eb1e087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_images = glob(f\"{landsat_chip_dir}/**/*.tiff\", recursive=True)\n",
    "first_l8_img = rioxarray.open_rasterio(landsat_images[0])\n",
    "first_l8_img.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37581811-0f21-4838-9541-db84688032f6",
   "metadata": {},
   "source": [
    "Shutdown the Dask client to cleanup cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d643476-917b-484b-a041-8f3c94d12c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c402a-81e6-46fb-bbb5-2c5fc79c8884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
